{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 14404,
     "status": "ok",
     "timestamp": 1638633703578,
     "user": {
      "displayName": "葉之晴",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15384545674565013435"
     },
     "user_tz": -480
    },
    "id": "Pnnw5JqDYcuL"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import jieba\n",
    "from gensim import corpora,models,similarities\n",
    "from collections import defaultdict\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1638633703578,
     "user": {
      "displayName": "葉之晴",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15384545674565013435"
     },
     "user_tz": -480
    },
    "id": "SCpfaJdDwyIK"
   },
   "outputs": [],
   "source": [
    "train_path = \"/Users/jhihchingyeh/Final Project/Dataset/stage1/\"\n",
    "test_path = \"/Users/jhihchingyeh/Final Project/Dataset/stage3/\"\n",
    "\n",
    "train_txtpath = \"dataTrainComplete/\"\n",
    "test_txtpath = \"dataPrivateComplete/\"\n",
    "\n",
    "place = [\"臺北\", \"新北\", \"桃園\", \"臺中\", \"臺南\", \"高雄市\",\n",
    "             \"新竹\", \"苗栗\", \"彰化\", \"南投\", \"雲林\", \"嘉義\", \"屏東\",\n",
    "             \"宜蘭\", \"花蓮\", \"臺東\", \"澎湖\", \"金門\", \"連江\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1638633703578,
     "user": {
      "displayName": "葉之晴",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15384545674565013435"
     },
     "user_tz": -480
    },
    "id": "8XgxsfvbwqIk"
   },
   "outputs": [],
   "source": [
    "# Read Data\n",
    "def read_text(path, txtpath):\n",
    "    # 1. txt\n",
    "    # Construct an empty dataframe to store txt data\n",
    "    df = pd.DataFrame(columns = [\"ID\", \"text\"])\n",
    "    k = 0\n",
    "    # Read txt and Store into df\n",
    "    for i in range(1402):\n",
    "        text = []\n",
    "        txt_name = str(i) + \".txt\"\n",
    "        txt_path = path + txtpath + txt_name\n",
    "        try:\n",
    "            f = open(txt_path, 'r')\n",
    "            text = f.read()\n",
    "            f.close\n",
    "            df.loc[k, 'ID'] = i\n",
    "            df.loc[k, 'text'] = text\n",
    "            k = k + 1\n",
    "        except:\n",
    "            pass\n",
    "    # Change to array\n",
    "    arr_df = np.array(df[\"text\"])\n",
    "\n",
    "\n",
    "    # 2. Keyword.xlsx\n",
    "    # Read excel\n",
    "    key_chem = pd.read_excel(path+\"Keywords/02chem.list.xlsx\", header=None, index_col=False)\n",
    "    key_crop = pd.read_excel(path+\"Keywords/02crop.list.xlsx\", header=None, index_col=False)\n",
    "    key_pest = pd.read_excel(path+\"Keywords/02pest.list.xlsx\", header=None, index_col=False)\n",
    "    # Merge them\n",
    "    frames = [key_chem, key_crop, key_pest]\n",
    "    keyword = pd.concat(frames, axis=0)\n",
    "    \n",
    "\n",
    "    # 3. Train Label.csv\n",
    "    # Test data does not have label\n",
    "    try: \n",
    "        label_path = path + \"TrainLabel.csv\"\n",
    "        label = pd.read_csv(label_path)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return df, keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the keyword by the shortest synonym\n",
    "def replace_keyword(df, keyword):\n",
    "    # Make and sort keyword lists\n",
    "    sort_keyword_list = []\n",
    "    num_cols = keyword.shape[1]\n",
    "    for ind, row in keyword.iterrows():\n",
    "        temp = [x for x in list(row) if pd.isnull(x) == False]\n",
    "        temp.sort(key=len, reverse=True)\n",
    "        sort_keyword_list.append(temp)\n",
    "    sort_keyword_list = sorted(sort_keyword_list, key=lambda x: len(x[0]), reverse=True)\n",
    "    \n",
    "    # Replace \n",
    "    for i in range(len(df)):\n",
    "        # Replace 台 into 臺\n",
    "        if \"台\" in df[\"text\"][i]:\n",
    "            df[\"text\"][i] = df[\"text\"][i].replace(\"台\", \"臺\")\n",
    "        # Replace the keyword by the shortest synonym\n",
    "        for j in range(len(sort_keyword_list)):\n",
    "            for k in sort_keyword_list[j][1:]:\n",
    "                key = str(k)\n",
    "                if key in df[\"text\"][i]:\n",
    "                    #print(key, sort_keyword_list[j][0])\n",
    "                    df[\"text\"][i] = df[\"text\"][i].replace(key, sort_keyword_list[j][0])\n",
    "    return df, sort_keyword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split df dataframe to smaller one(keyword_df) by Keyword  \n",
    "def keyword_df(df, sort_keyword_list, path):\n",
    "    for i in range(len(sort_keyword_list)):\n",
    "        # Build keyword_n dataframe to store the acticle with the same keyword\n",
    "        df_temp_keyword = pd.DataFrame(columns = [\"ID\", \"text\"])\n",
    "        for j in range(len(df)):\n",
    "            if sort_keyword_list[i][0] in df[\"text\"][j]:\n",
    "                df_temp_keyword = df_temp_keyword.append(df[j:j+1], ignore_index=True) \n",
    "        # Save each dataframe\n",
    "        df_temp_keyword.to_csv(path+\"keyword_df/keyword_\"+str(i)+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split keyword_df dataframe to smaller one(place_keyword_df) by place  \n",
    "def place_keyword_df(sort_keyword_list, path):\n",
    "    place = [\"臺北\", \"新北\", \"桃園\", \"臺中\", \"臺南\", \"高雄市\",\n",
    "             \"新竹\", \"苗栗\", \"彰化\", \"南投\", \"雲林\", \"嘉義\", \"屏東\",\n",
    "             \"宜蘭\", \"花蓮\", \"臺東\", \"澎湖\", \"金門\", \"連江\"]\n",
    "    \n",
    "    for i in range(len(sort_keyword_list)):\n",
    "        each_keyword_df = pd.read_csv(path+\"keyword_df/keyword_\"+str(i)+\".csv\")\n",
    "        for j in range(len(place)):\n",
    "            # Build place_n dataframe to store the acticle with the same keyword and place\n",
    "            df_temp_place = pd.DataFrame(columns = [\"ID\", \"text\"])\n",
    "            for k in range(len(each_keyword_df)):\n",
    "                if place[j] in each_keyword_df[\"text\"][k]:\n",
    "                    df_temp_place = df_temp_place.append(each_keyword_df[k:k+1], ignore_index=True)\n",
    "            # Save each dataframe\n",
    "            df_temp_place.to_csv(path+\"place_keyword_df/\"+\"key\"+str(i)+\"_place\"+str(j)+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1638633703579,
     "user": {
      "displayName": "葉之晴",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15384545674565013435"
     },
     "user_tz": -480
    },
    "id": "y1gR-4yx4qWg"
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def prediction(submission, m, smaller_df, all_list, dictionary, prob):\n",
    "    # Calculate the similarity\n",
    "    for k in range(len(smaller_df)):\n",
    "        # Sparse Matrix\n",
    "        new_vec = dictionary.doc2bow(smaller_df[\"jieba\"][k])\n",
    "        corpus = [dictionary.doc2bow(ii) for ii in all_list]\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        featureNUM = len(dictionary.token2id.keys())\n",
    "        index = similarities.SparseMatrixSimilarity(tfidf[corpus],num_features=featureNUM)\n",
    "        sim = index[tfidf[new_vec]]\n",
    "        # Similarity Probability\n",
    "        for s in range(len(sim)):\n",
    "            if (sim[s]<1)&(sim[s]>prob) == True:\n",
    "                if (str(smaller_df[\"ID\"][k]) != str(smaller_df[\"ID\"][s])):\n",
    "                    #print(smaller_df[\"ID\"][k], smaller_df[\"ID\"][s])\n",
    "                    submission.loc[m, \"Test\"] = smaller_df[\"ID\"][k]\n",
    "                    submission.loc[m, \"Reference\"] = smaller_df[\"ID\"][s]\n",
    "                    m += 1\n",
    "    return submission, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jieba Dictionary\n",
    "def jieba_dict(smaller_df):\n",
    "    # ignore warning\n",
    "    warnings.filterwarnings('ignore')\n",
    "    warnings.warn('DelftStack')\n",
    "    warnings.warn('Do not show this message')\n",
    "    all_list = []\n",
    "    smaller_df[\"jieba\"] = None\n",
    "    for i in range(len(smaller_df)):\n",
    "        data_ = []\n",
    "        list_ = []\n",
    "        data_ = jieba.cut(smaller_df[\"text\"][i])\n",
    "        for j in data_:\n",
    "            list_.append(j)\n",
    "        smaller_df[\"jieba\"][i] = list_\n",
    "    # Calculate the Frequency of Term\n",
    "    all_list = smaller_df['jieba'].values.tolist()\n",
    "    frequency = defaultdict(int)\n",
    "    for m in all_list:\n",
    "        for n in m:\n",
    "            frequency[n] += 1\n",
    "            \n",
    "    # Build the Dictionary\n",
    "    dictionary = corpora.Dictionary(all_list)\n",
    "    return smaller_df, all_list, dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1638633703579,
     "user": {
      "displayName": "葉之晴",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "15384545674565013435"
     },
     "user_tz": -480
    },
    "id": "inJWz-E43V4I"
   },
   "outputs": [],
   "source": [
    "# Model Prediction by place and keyword\n",
    "def term_model_pred(path, sort_keyword_list, place, prob):\n",
    "    # Save answer\n",
    "    m = 0\n",
    "    submission = pd.read_csv(path+\"submission_example.csv\")\n",
    "    # Read each smaller dataframe\n",
    "    for a in range(len(sort_keyword_list)):\n",
    "        for b in range(len(place)):\n",
    "            smaller_df = pd.read_csv(path+\"place_keyword_df/\"+\"key\"+str(a)+\"_place\"+str(b)+\".csv\")\n",
    "            # - jieba\n",
    "            smaller_df, all_list, dictionary = jieba_dict(smaller_df)\n",
    "            # - Prediction\n",
    "            submission, m = prediction(submission, m, smaller_df, all_list, dictionary, prob)\n",
    "            \n",
    "    return submission, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Prediction by similarity\n",
    "def sim_model_pred(path, df, prob, submission, m):\n",
    "    # - jieba\n",
    "    df, all_list, dictionary = jieba_dict(df)\n",
    "\n",
    "    # - Prediction\n",
    "    submission, m = prediction(submission, m, df, all_list, dictionary, prob)\n",
    "            \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, keyword = read_text(test_path, test_txtpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, sort_keyword_list = replace_keyword(df, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df(df, sort_keyword_list, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_keyword_df(sort_keyword_list, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/2s/tlvgbrd16jx_vcmf7z8m3sbm0000gn/T/jieba.cache\n",
      "Loading model cost 0.425 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "submission, m = term_model_pred(test_path, sort_keyword_list, place, 0.35) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = sim_model_pred(test_path, df, 0.7, submission, m) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_no_duplicates = submission.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_no_duplicates = submission_no_duplicates.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test</th>\n",
       "      <th>Reference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>232</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>253</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>320</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>320</td>\n",
       "      <td>328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>320</td>\n",
       "      <td>379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>1348</td>\n",
       "      <td>1291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>676</th>\n",
       "      <td>1348</td>\n",
       "      <td>1294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>1348</td>\n",
       "      <td>1356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>1356</td>\n",
       "      <td>1335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>679</th>\n",
       "      <td>1356</td>\n",
       "      <td>1348</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>680 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Test Reference\n",
       "0     232       253\n",
       "1     253       232\n",
       "2     320       322\n",
       "3     320       328\n",
       "4     320       379\n",
       "..    ...       ...\n",
       "675  1348      1291\n",
       "676  1348      1294\n",
       "677  1348      1356\n",
       "678  1356      1335\n",
       "679  1356      1348\n",
       "\n",
       "[680 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_no_duplicates.to_csv(test_path+\"result.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, keyword = read_text(train_path, train_txtpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, sort_keyword_list = replace_keyword(df, keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_df(df, sort_keyword_list, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_keyword_df(sort_keyword_list, train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_acc(submission):\n",
    "    # Label\n",
    "    label = pd.read_csv(r\"/Users/jhihchingyeh/Final Project/Dataset/stage1/TrainLabel.csv\")\n",
    "    for i in range(len(label)):\n",
    "        if int(label[\"Test\"][i]) > int(label[\"Reference\"][i]):\n",
    "            temp = label[\"Test\"][i]\n",
    "            label[\"Test\"][i] = label[\"Reference\"][i]\n",
    "            label[\"Reference\"][i] = temp\n",
    "    \n",
    "    # Prediction\n",
    "    # Choose the best prob\n",
    "    prob = 0.6\n",
    "    while prob < 1:\n",
    "        submission, m = term_model_pred(train_path, sort_keyword_list, place, prob)\n",
    "        for i in range(len(submission)):\n",
    "            if int(submission[\"Test\"][i]) > int(submission[\"Reference\"][i]):\n",
    "                temp = submission[\"Test\"][i]\n",
    "                submission[\"Test\"][i] = submission[\"Reference\"][i]\n",
    "                submission[\"Reference\"][i] = temp\n",
    "        \n",
    "        # F1-score\n",
    "        tp = 0\n",
    "        fp = 0\n",
    "        fn = 0\n",
    "        Precision = 0\n",
    "        Recall = 0\n",
    "        F1 = 0\n",
    "        for m in range(len(submission)):\n",
    "            testID = submission[\"Test\"][m]\n",
    "            referenceID = submission[\"Reference\"][m]\n",
    "            label_mask = label[label[\"Test\"] == testID]\n",
    "            label_mask = label_mask.reset_index(drop=True)\n",
    "            if len(label_mask) == 0:\n",
    "                fp = fp + 1\n",
    "            else:\n",
    "                for n in range(len(label_mask)):\n",
    "                    if label_mask[\"Reference\"][n] == referenceID:\n",
    "                        tp = tp + 1\n",
    "                    else:\n",
    "                        fn = fn + 1\n",
    "\n",
    "\n",
    "\n",
    "        Precision = tp /(tp+fp)\n",
    "        Recall = tp /(tp+fn)\n",
    "        F1 = 2 / ((1/Precision) + (1/Recall))\n",
    "        \n",
    "        print(\"prob: \", prob)\n",
    "        print(\"F1: \", F1)\n",
    "        prob = prob +0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPNedq0f5PU9N4XuHJknGnt",
   "collapsed_sections": [],
   "name": "try_model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
